#!/usr/bin/env python3
"""
Debug script for Ollama streaming
Run this to test direct streaming connection to Ollama
"""

import os
import json
import time
import requests
from dotenv import load_dotenv

def test_ollama_streaming():
    """Test Ollama streaming directly"""
    print("ğŸ” Testing Ollama Streaming Connection...")
    print("=" * 60)
    
    # Load environment
    load_dotenv()
    
    # Configuration
    ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    ollama_model = os.getenv("OLLAMA_MODEL", "")
    ollama_timeout = int(os.getenv("OLLAMA_TIMEOUT", "600"))
    
    print(f"ğŸŒ Server: {ollama_url}")
    print(f"ğŸ¤– Model: {ollama_model}")
    print(f"â±ï¸  Timeout: {ollama_timeout}s ({ollama_timeout//60}m)")
    print()
    
    if not ollama_model:
        print("âŒ OLLAMA_MODEL not configured in .env")
        return False
    
    # Simple test prompt
    test_prompt = """Create a simple cube using CadQuery. Respond with JSON containing:
{
    "intention_type": "creation",
    "response_text": "Creating a 10mm cube",
    "execution_plan": {
        "id": "cube_001",
        "description": "Simple cube creation",
        "cadquery_code": "import cadquery as cq\\nresult = cq.Workplane().box(10, 10, 10)"
    }
}"""
    
    payload = {
        "model": ollama_model,
        "prompt": test_prompt,
        "stream": True,
        "options": {
            "temperature": 0.1,
            "num_predict": 1000,
            "top_p": 0.9,
            "top_k": 40
        }
    }
    
    print(f"ğŸ“ Sending test prompt ({len(test_prompt)} chars)...")
    print(f"ğŸ¯ Prompt preview: '{test_prompt[:100]}...'")
    print()
    
    start_time = time.time()
    
    try:
        print(f"ğŸš€ Making request to {ollama_url}/api/generate...")
        
        response = requests.post(
            f"{ollama_url}/api/generate",
            json=payload,
            timeout=ollama_timeout,
            stream=True
        )
        
        if response.status_code != 200:
            print(f"âŒ HTTP Error {response.status_code}")
            print(f"Response: {response.text}")
            return False
        
        print(f"âœ… Connection established (status: {response.status_code})")
        print("ğŸ”„ Starting to receive streaming data...")
        print("-" * 60)
        
        # Process streaming response
        full_response = ""
        chunk_count = 0
        last_log_time = time.time()
        
        for line in response.iter_lines():
            if line:
                chunk_count += 1
                current_time = time.time()
                elapsed = current_time - start_time
                
                try:
                    chunk_data = json.loads(line.decode('utf-8'))
                    
                    # Log every chunk for debugging
                    print(f"ğŸ“¦ Chunk {chunk_count} ({elapsed:.1f}s): {json.dumps(chunk_data)}")
                    
                    # Check for errors
                    if 'error' in chunk_data:
                        print(f"âŒ Error in chunk: {chunk_data['error']}")
                        return False
                    
                    # Add partial response
                    if 'response' in chunk_data:
                        partial_response = chunk_data['response']
                        full_response += partial_response
                        
                        if chunk_count == 1:
                            print(f"ğŸ‰ First response received!")
                    
                    # Check if done
                    if chunk_data.get('done', False):
                        total_time = time.time() - start_time
                        print("-" * 60)
                        print(f"âœ… Stream completed!")
                        print(f"â±ï¸  Total time: {total_time:.1f}s")
                        print(f"ğŸ“ˆ Total chunks: {chunk_count}")
                        print(f"ğŸ“ Response length: {len(full_response)} chars")
                        break
                        
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  Invalid JSON chunk: {line[:100]}...")
                    print(f"   Error: {e}")
                    continue
        
        print()
        print("ğŸ“„ FULL RESPONSE:")
        print("=" * 60)
        print(full_response)
        print("=" * 60)
        
        if not full_response:
            print("âŒ No response received!")
            return False
        
        # Try to parse as JSON
        try:
            response_json = json.loads(full_response)
            print("âœ… Response is valid JSON")
            print(f"ğŸ¯ Intention type: {response_json.get('intention_type', 'N/A')}")
        except json.JSONDecodeError:
            print("âš ï¸  Response is not valid JSON (this might be expected for some models)")
        
        print()
        print("ğŸ‰ Streaming test completed successfully!")
        return True
        
    except requests.exceptions.Timeout:
        elapsed = time.time() - start_time
        print(f"â° Request timed out after {elapsed:.1f}s")
        print(f"   Configured timeout: {ollama_timeout}s")
        print("ğŸ’¡ Try increasing OLLAMA_TIMEOUT or using a smaller model")
        return False
        
    except requests.exceptions.ConnectionError as e:
        print(f"ğŸ”Œ Connection error: {e}")
        print("ğŸ’¡ Make sure Ollama is running with: ollama serve")
        return False
        
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"ğŸ’¥ Unexpected error after {elapsed:.1f}s: {e}")
        return False

def check_ollama_status():
    """Quick check of Ollama status"""
    print("ğŸ” Checking Ollama Status...")
    
    load_dotenv()
    ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    
    try:
        # Check if service is running
        response = requests.get(f"{ollama_url}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [m.get('name', '') for m in models]
            print(f"âœ… Ollama is running at {ollama_url}")
            print(f"ğŸ“¦ Available models: {model_names}")
            return True
        else:
            print(f"âŒ Ollama returned status {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Cannot connect to Ollama: {e}")
        print("ğŸ’¡ Start Ollama with: ollama serve")
        return False

if __name__ == "__main__":
    print("ğŸ§ª Ollama Streaming Debug Tool")
    print("=" * 60)
    
    # First check if Ollama is running
    if not check_ollama_status():
        exit(1)
    
    print()
    
    # Test streaming
    success = test_ollama_streaming()
    
    if success:
        print("\nğŸ‰ All tests passed! Ollama streaming is working.")
    else:
        print("\nâŒ Tests failed. Check the output above for details.")
        exit(1) 